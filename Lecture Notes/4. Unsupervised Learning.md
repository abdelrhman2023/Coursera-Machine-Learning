## K-means clustering algorithm

**Steps**

1. Initialize randomly K points called cluster centroids.
2. **Cluster assignment**: group dataset examples to K groups based on their distance to the cluster centroid.
3. **Move Centroids**: move the centroid C<sub>k</sub> to the mean of cluster k.
4. Repeat from step 2 till convergence, or till the centroids don't move anymore.

**Cost function (Distortion Function)**

1/m * sum(|| X<sup>(i)</sup> - µ<sub>k</sub> || <sup>2</sup>)


**Avoid local optima by**

1. Run K-mean multiple times (50 to 1000 times) with different random initialization.
2. Pick the model with the lowest cost function (Distortion / Error).

**Choosing the number of clusters**

- This is usually picked manually by plotting the data, or by the business logic like when clustering T-shirt sizes.
- The elbow method: Plot the distortion function against K and find the elbow of the curve.

## Dimensionality Reduction

Reducing linearly dependent features, this will help with visualizing the data and will decrease the learning time.<br>
The various methods used for dimensionality reduction include:

- Principal Component Analysis (PCA)
- Linear Discriminant Analysis (LDA)
- Generalized Discriminant Analysis (GDA)

### Principal Component Analysis (PCA)

Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.<br>
PCA minimizes the projection of the points X<sup>(i)</sup> onto a hyperplane, that will help in reducing this dimension.

**Steps**

1. Perform Standardization (Mean normalization).
2. Compute the covariance matrix = 1/m * (X' * X).
3. Compute eigen vectors (U) of the matrix (Singular value decomposition).
4. U<sub>Reduced</sub> = U[: ,:K], where K = number of dimensions desired.
5. Z = U<sub>Reduced</sub><sup>T</sup> * X.

The third step is done easily in NumPy with function svd() U, S, V = np.linalg.svd(CovarianceMatrix)

```python
# U: The eigenvectors, representing the computed principal components of X. 
# S: A vector of size n, containing the singular values for each principal component.

U, S, V = np.linalg.svd(CovarianceMatrix)
```

**Decompression of data**

X<sub>Approx</sub> = U<sub>Reduced</sub> * Z

**Choosing K (Number of principle components)**

1. Calculate the average squared projection error = 1/m * sum( || X<sup>(i)</sup> - X<sub>Approx</sub><sup>(i)</sup> ||<sup>2</sup> )
2. Calculate total variation in the data = 1/m * sum( || m<sup>(i)</sup> ||<sup>2</sup> )

Try from K = 1, and go up till the following equation is satisfies <br>
average squared projection error / total variation in the data < 0.01 (99% of the variance is retained)

or from  the singular values matrix S returned from svd()<br>
sum<sub>from i = 1 to K</sub>((S<sub>ii</sub>) / sum<sub>from i = 1 to n</sub>(S<sub>ii</sub>) > 0.99

## Anomaly detection

Anomaly detection (or outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, anomalous data can be connected to some kind of problem or rare event such as e.g. bank fraud, medical problems, malfunctioning equipment etc.

When to Use ?

- Very small number of positive examples.
- large number of negative examples.

**Gaussian (Normal) Distribution**

X ~ N(µ, σ<sup>2</sup>) = p(X; µ, σ<sup>2</sup>)

**Steps**

1. Find µ for each feature X.
2. Find σ<sup>2</sup> for each feature X.
3. Identify anomalies if p(X<sub>new</sub>; µ, σ<sup>2</sup>) &lt; ε = p(X<sup>1</sup>; µ, σ<sup>2</sup>) * ... * p(X<sup>n</sup>; µ, σ<sup>2</sup>) &lt; ε.
4. If we have labeled data 
	1. Find the probability for each training example (Density estimation) = p(X<sup>1</sup>; µ, σ<sup>2</sup>) * ... * p(X<sup>n</sup>; µ, σ<sup>2</sup>).
	2. Use the labels to pick ε, Epsilon, by trying different values and choosing the model with the highest F-Score.

**Choosing features**

- Plot the feature to see if its conforming to a normal distribution, if not, try to use transformation like log(), Power, ... to conform it to a normal distribution.
- Error analysis, manually see what is misclassified (False positive examples).
- Try to come of with ratio or relational features, ex: CPU load / Network traffic, This captures when a computer has a high CPU load and low network traffic, which defines it as an anomaly.

**Multivariate Gaussian Distribution**

This is used when we have features that depend on each other, linearly dependant, this automatically captures ratio features, which allows the graph of the probability boundary &lt; ε. to be angled to fit the data, instead of being aligned with the axes.

The only change in steps is instead of finding the Density estimation = p(X<sup>1</sup>; µ, σ<sup>2</sup>) * ... * p(X<sup>n</sup>; µ, σ<sup>2</sup>),<br>
we find the p(X; µ, σ<sup>2</sup>), which is the probability of all features X given µ and σ<sup>2</sup>.

This also means that this model will be slow with large number of features.


## Recommender System

### Content based recommender system

- Perform linear regression for each user.
- Prediction = Θ<sup>T</sup> * X, where Θ is for each user and X are the features of product or a movie.

### Collaborative filtering

We set the number of features of a movie or a product without any real numbers or meaning to the features, and train a model that will optimize values of the features and values of theta given some user input.<br>
So the model depends on users that have similar taste to model X and Θ. 

Objective: Given X, estimate Θ and given Θ estimate X

Cost function: minimize J(Θ, X).

**Steps**

1. Initialize X and Θ to small random values for symmetry breaking.
2. Minimize J(Θ, X) using gradient.
3. For a user with parameters Θ and a product with features X, predict rating using Θ<sup>T</sup> * X.

**Low rank matrix factorization**

When learning features X, you can find similar products by finding a small value of || X<sup>a</sup> - X<sup>b</sup> ||.

**Mean Normalization**

This is very helpful, because now the system will recommend a product based on the average rating if no Θ's are defined for a user (new user).

- Find µ of each product.
- Set Y (All product ratings from all users) = Y - µ.
- For a user j on product i, predict Θ<sup>j T</sup> * X<sup>i</sup>  + µ<sup>i</sup>, this will add the average rating to the prediction which will give a default rating of the average rating instead of zero.



















