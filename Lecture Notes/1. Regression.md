## Linear Regression

Linear regression is a linear model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).

When there is a single input variable (x), the method is referred to as univariate linear regression, The objective here is to fit a line function to the dataset. When there are multiple input variables, the method is referred to as multivariate linear regression, 

The objective here is to fit a polynomial function to the dataset, we pick the degree of the polynomial according to the complexity of the shape of data, this is either shown by plotting the data which is easy with one or two features by using 2D or 3D graphs, or if the gradient descent algorithm converges to a high error.

###### NOTE: Even when picking a degree higher than the first degree, we still are doing linear regression modeling as the parameters are not raised to any higher power, we are just generating new features that corresponds to the polynomial terms.

## Logistic Regression

A classification model that predicts the probability of a certain class, the output of the dataset in this case belongs to a discrete range of values.
Binary classification is used when there are two classes in the output, of more than two classes are present then we use multiclass classification which uses the one vs all method by training multiple binary classification models and picking the highest probability as the output class.

The cost function uses a logarithmic function to calculate the error.

Increasing the degree of the polynomial functions determines the shape complexity of the decision boundary, not fitting the data like in linear regression. so it is more common to use high degree polynomials to determine the decision boundary.

**Threshold**

A value that is used to define the boundaries of the probability, so the output becomes positive when the probability exceeds the threshold value, and negative if the the probability is less than the threshold value.


----------


### Hypothesis function

A function that describes the relationship between the features, that will output the best estimate to the real output of the given dataset. in case of linear regression we use a polynomial function to the nth degree.

The objective is to find the best coefficients and y-intercept which are called **parameters** that minimizes the error between the hypothesis function and the real output of the given dataset.

### Cost function

A function that calculates the error of the hypothesis function giving us an accuracy measure of the model, commonly used function is the MSE (Mean Squared Error).

### Gradient descent

An iterative optimization algorithm for finding a local minimum of a differentiable function (The cost function). To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient of the function at the current point.

The slope of the function gives us the magnitude of the step as higher slopes means we are far from the local minimum and vice versa. and the sign of the slope gives us a direction to move towards, so at the step magnitude is high at first as we are far from the local minimum, and decreases over iterations as we get closer to the local minimum.

**Learning rate** is a coefficient of the gradient that is used to either scale up or scale down the gradient step.

If we just supply the gradients (Step size) for the current step and the cost of the current step (Error) in a function we can give this function to other advanced optimization algorithms.

##### Batch gradient descent 

As we are using the MSE function as the cost function in gradient descent, each parameter gradient is using all the training examples in the dataset.

### Monitoring the convergence of the cost function

We usually define the number of iterations (Epoch) but we can use an automatic convergence test b detecting if the gradient of the current step is less than Epsilon which is a very small number.

We can also plot the cost function across the iterations to observe the descent of the cost.If the plot diverges instead of converging then we are using a large learning rate which overshoots the local minimum.

### Normal equation

An analytical approach (not iterative) to Linear Regression with a Least Square Cost Function. We can directly find out the value of the parameters without using Gradient Descent. it does not need a learning rate parameter, but is slow with large number of features and is subject to non invertable matrices.

### Feature Scaling

Helps the gradient descent algorithm converge more quickly.

Z-score standardization: gets every feature to approximately be in (-1, 1) range, by subtracting the average and dividing by sigma (standard deviation).

### Regularization

Scales down the parameters of the hypothesis function to avoid overfitting the data, we add a coefficient (Lambda) to the square root of the sum of parameters.

which means if lambda is large the minimization algorithm will have to minimize the parameters to minimize the cost function, which will limit the contribution of the polynomial terms and avoid overfitting, but picking a high number as lambda might minimize the contribution to much which result in underfitting the data.


