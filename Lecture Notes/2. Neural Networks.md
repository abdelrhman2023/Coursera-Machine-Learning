## Neural Network

A computational learning system that uses a network of functions to understand and translate a data input of one form into a desired output.

Generating polynomial terms to a dataset with large number of features is impossible as the number of terms will roughly equal n^2 / 2 terms and with something like images as input the number of features will be very large.

An input layer is connected to a hidden layer with a set of weights then the output is converted to a probabilistic value with an activation function, the output of the hidden layer can either be connected to another hidden layer or by the output layer using another set of weights.

### Forward propagation

This is the process of calculating the value of the nodes at each layer going forward in the model till we get the output of the Neural network given a set of inputs by following these steps.

1. Initialize the weights to a random value to avoid problem of symmetry (Weights coming from a node will be equal after training if they started at zero before training).
2. Multiply the input by their respective weights to get the output at the hidden layer.
3. Convert the value to a probabilistic value by using an activation function like Sigmoid.
4. Repeat the process through all hidden layers.
5. Calculate the probability output at the output layer.

### Cost function 

The cost function uses a logarithmic function to calculate the error.

A regularization term is used to avoid overfitting the model to the dataset.


### Backpropagation

An algorithm that given the error at the output layer, calculates the gradient for each weight going backwards in layers as we propagate the error backwards by following these steps.

1. Calculate the error at the output layer which is the difference between the actual output and the output calculated by the forward propagation step.
2. Calculate the error at the hidden layer that is before the output.
	- The error at the hidden layer = (W * OutputError) * F'(HiddenLayerOutput), where F is the activation function.
	- Calculate the variations of the weights = Output * Error
	- Calculate the gradients = 1/m * Variation + (Learning rate * weights)
3. Repeat the error calculation at the previous hidden layer till you reach the input layer.

The cost function is not of a convex shape, so we minimize to a local minimum not the global minimum, but it is not a problem as the local minimums are not largely different form each other.

### Gradient Checking

Errors done during Backpropagation will result in low accuracy, so it is not apparent that a bug is present in the code.

A numerical estimate of the gradients used for debugging purposes which is calculate by defining a small number, Epsilon, and calculating the cost function of the weights + Epsilon and weights - Epsilon and dividing the sum by 2 * Epsilon.

This calculation is slow, so it is used only for checking the gradient calculation in the backpropagation algorithm for a few iterations only, once the gradients are verified, the checking is turned off and the backpropagation algorithm is used to train the model for the full number of iterations.