## Support vector machine

It is also called large margin classifier as it chooses the largest margin of the decision boundary.<br>
It removes the lambda parameter and introduces the C parameter which is equal to 1 / Lambda.<br>

**Support vectors** are the points closest to decision boundary which are hard to classify.

### The math simplified intuition

Given C is large the optimization tries to minimize ||Θ||<sup>2</sup>, <br>
p = the projection of X<sup>(i)</sup> onto the vector Θ, <br>
X<sup>T</sup> * Θ = p<sup>(i)</sup> * ||Θ||<br>
To minimize ||Θ|| it has to maximize p, which means that it has to maximize the distance of the points X projected on to the theta vector that acts as the decision boundary.

### Kernel Trick

By introducing new features, we can convert a non linear space to a linearly separable space.

### Kernels

Adapts SVM to develop complex non linear classifiers.

Steps:

1. Define a similarity function (Kernel function) that measures the proximity to an arbitrary landmark point L.<br>
Commonly used in the Gaussian function, which outputs 1 if X is similar to L and 0 if they are far from each other.
2. This landmark point will define an area around it that defines if X is similar to the landmark i.e = 1 or outside the area i.e = 0.
3. Choose each feature X as a landmark and compute a new feature based on the similarity of all examples X to this landmark. so we will create F1, F2, F3, ... , Fm.
4. The union of all the areas defined by the landmarks will define a complex shape that will act as the decision boundary.

### Parameters

**C**

**When C is large** it becomes sensitive to outliers, and may introduce a high variance problem.<br>
**When C is small** it ignores the outlier examples, and may introduce a high bias problem.

**σ<sup>2</sup>** 

value will decide how big the area will be around the landmarks, which will expand or shrink the decision boundary.<br>
**When σ<sup>2</sup> is large** Features F<sub>i</sub> vary smoothly, and may introduce a high bias problem.<br>
**When σ<sup>2</sup> is small** Features F<sub>i</sub> vary less smoothly, and may introduce a high variance problem.


#### NOTES

- Perform feature scaling when using a Gaussian kernal.
- Some other kernels <br>
Polynomial kernels, String Kernels, Chi square kernels
- No Kernel = Linear kernel which is like Logistic regression.
- Kernels measures similarity, so choosing them is based on the type of data.
- SVM is a convex function, so it will always converge to global minimum.
- SVM does not output a probabilistic value, it is either 1 or 0.